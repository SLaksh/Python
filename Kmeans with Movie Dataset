# -*- coding: utf-8 -*-
"""KMeansClusteringWIthMovie Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Sm626yT2a1Gr7PLCMCISSlAwf_AcUSr

# **Understanding Document Clustering**

Clustering is one of the most important Unsupervised Machine Learning Techniques.  Clustering techniques have been studied in depth over the years and there are some very powerful clustering algorithms available. For this tutorial, we will be working with a movie dataset containing movie plot, cast, genres and related other information. We will be working with K-Means
"""

import pandas as pd

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('tmdb_5000_movies.csv.gz', 
                 compression='gzip')
df.info()

df.head()

df = df[['title', 'tagline', 'overview', 'genres', 'popularity']]
df.tagline.fillna('', inplace=True)
df['description'] = df['tagline'].map(str) + ' ' + df['overview']
df.dropna(inplace=True)
df.info()

""" **The fillna() method replaces the NULL values with a specified value. The fillna() method returns a new DataFrame object unless the inplace parameter is set to True , in that case the fillna() method does the replacing in the original DataFrame instead.**"""

df.head()

"""## **Text pre-processing**"""

import nltk
import re
import numpy as np
import nltk
nltk.download('stopwords')
nltk.download('punkt')

stop_words = nltk.corpus.stopwords.words('english')

def normalize_document(doc):
    # lower case and remove special characters\whitespaces
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I|re.A)
    doc = doc.lower()
    doc = doc.strip()
    # tokenize document
    tokens = nltk.word_tokenize(doc)
    # filter stopwords out of document
    filtered_tokens = [token for token in tokens if token not in stop_words]
    # re-create document from filtered tokens
    doc = ' '.join(filtered_tokens)
    return doc

normalize_corpus = np.vectorize(normalize_document)

norm_corpus = normalize_corpus(list(df['description']))
len(norm_corpus)

"""## **Extract TF-IDF Features**"""

from sklearn.feature_extraction.text import CountVectorizer

stop_words = stop_words + ['one', 'two', 'get']
cv = CountVectorizer(ngram_range=(1, 2), min_df=10, max_df=0.8, stop_words=stop_words)
cv_matrix = cv.fit_transform(norm_corpus)
cv_matrix.shape

"""## **CountVectorizer. Convert a collection of text documents to a matrix of token counts.  CountVectorizer will tokenize the data and split it into chunks called n-grams, of which we can define the length by passing a tuple to the ngram_range argument**

The sklearn.feature_extraction module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image. We keep text tokens in our normalized text and extract bag of words count based features for unigrams and bigrams such that each feature occurs in at least 10 documents and at most 80% of the documents using the terms min_df and max_df. We can see that
we have a total of 4,800 rows for the 4,800 movies and a total of 3,012 features for each movie. Now that we have our features and documents ready, we can start the clustering analysis.

# **Cluster Movies using K-Means**
"""

from sklearn.cluster import KMeans

NUM_CLUSTERS = 6
km = KMeans(n_clusters=NUM_CLUSTERS, max_iter=10000, n_init=50, random_state=42).fit(cv_matrix)
##   KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd') is the syntax
##   n_initint, default=10 is nothing but -  Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.
##   max_iterint, default=300 - Maximum number of iterations of the k-means algorithm for a single run.
##   random_stateint, RandomState instance or None, default=None - Determines random number generation for centroid initialization. Use an int to make the randomness deterministic

km

###  A Counter is a container that keeps track of how many times equivalent values are added. 
## It can be used to implement the same algorithms for which bag or multiset data structures are commonly used in other languages
from collections import Counter
Counter(km.labels_)

df['kmeans_cluster'] = km.labels_

movie_clusters = (df[['title', 'kmeans_cluster', 'popularity']]
                  .sort_values(by=['kmeans_cluster', 'popularity'], 
                               ascending=False)
                  .groupby('kmeans_cluster').head(20))
movie_clusters = movie_clusters.copy(deep=True)

feature_names = cv.get_feature_names()
topn_features = 15
ordered_centroids = km.cluster_centers_.argsort()[:, ::-1]

# get key features for each cluster
# get movies belonging to each cluster
for cluster_num in range(NUM_CLUSTERS):
    key_features = [feature_names[index] 
                        for index in ordered_centroids[cluster_num, :topn_features]]
    movies = movie_clusters[movie_clusters['kmeans_cluster'] == cluster_num]['title'].values.tolist()
    print('CLUSTER #'+str(cluster_num+1))
    print('Key Features:', key_features)
    print('Popular Movies:', movies)
    print('-'*80)
